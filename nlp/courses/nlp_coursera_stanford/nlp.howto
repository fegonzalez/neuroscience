################################################################################
# INDEX
################################################################################

1) Lecture01_1.Basic_Text_Processing 
   1.1) Regular Expressions
   1.2) Regular_Expressions_in_Practical_NLP
   1.3) Word Tokenization
   	a) Simple Tokenization in Unix
   1.4) Word Normalization and Stemming
   	a) Lemmatization
   1.5) Sentence Segmentation
1_2) Lecture01.2: Edit_Distance
   1.6) Defining_Minimum_Edit_Distance
   1.7) Computing_Minimum_Edit_Distance
   1.8) Backtrace_for_Computing_Alignments
   1.9) Weighted_Minimum_Edit_Distance
   1.10) Minimum_Edit_Distance_in_Computational_Biology

2) Lecture02.Language Modeling
   2.1) Introduction to N-grams (8:41)
   2.2) Estimating N-gram Probabilities (9:38)
   2.3) Evaluation and Perplexity (11:09)
   2.4) Generalization and Zeros (5:15)
   2.5) Add-One (Laplace) smoothing (6:30)
   2.6) Interpolation, Backoff, and Web-Scale LMs (10:25)
   	2.6.1) Interpolation
   	2.6.2) Unknown words: Open versus closed vocabulary tasks
   2.7) Good-Turing Smoothing (15:35)   
   	2.7.2) Ney et al. Good Turing Intuition:
	2.7.3) Good-Turing complications:
   2.8) Kneser-Ney Smoothing (8:59)
   	2.8.end) N-gram Smoothing Summary
	
   Recap
   ● N-gram Language Model
   ● Estimation
   ● Smoothing
      – Laplace
      – Good-Turing
      – Kneser-Ney
   

3) Lecture03.Spelling_Correction
   3.1) The Spelling Correction Task (5:39)
   3.2) The Noisy Channel Model of Spelling (19:30)
   3.3) Real-Word Spelling Correction (9:19)
   3.4) State of the Art Systems (7:10)


Annex) References



################################################################################
Lecture01_1.Basic_Text_Processing 
################################################################################

#-------------------------------------------------------------------------------
# 1.3) Word Tokenization
#-------------------------------------------------------------------------------

• Type: an element of the vocabulary.
• Token: an instance of that type in running text.
  V = vocabulary = set of types (|V| = size)
  N = number of tokens

# Simple Tokenization in Unix (get V)
tr 'A-Z' 'a-z' <materials/shakes.txt  # to low-case
   | tr -sc 'A-Za-z' '\n'             # remove spaces & print one token per line
   | sort                             # sort alphabet.
   | uniq -c                          # merge & count each 'Type' (V)
   | sort -n -r | less                # sort by frequency


Issues is tokenization:
• I'm, isn't
• state-of-the-art -> state of the art ?
• lowercase -> lower-case, lowercase, lower case ?
• Languages issues: Chinese & Japanese has no spaces between words, ...


#-------------------------------------------------------------------------------
1.4) Word Normalization and Stemming
#-------------------------------------------------------------------------------

a) Lemmatization: reduce inflections or variant forms to base form.
   • am, are, is -> be
   • car, cars, car's -> car

b) Stemming

* Morphemes: stems (core) + affixes (prefix & suffix)
   
* Stemming = reduce terms to their stems in info. retrieval = chopping affixes.
  – language dependent
  – e.g., automate(s), automatic, automation -> all reduced to automat.
  - e.g. (English) Porter's algorithm.


#-------------------------------------------------------------------------------
1.5) Sentence Segmentation
#-------------------------------------------------------------------------------

Find the final of sentences (?, !, .): use binary classifiers (decision trees).



################################################################################
Lecture01_1.Basic_Text_Processing 
################################################################################

#-------------------------------------------------------------------------------
# 1.6) Defining_Minimum_Edit_Distance
#-------------------------------------------------------------------------------

def) Minimum number of editing operations (insertion, deletion, substitution),
needed to transform one into the other.

       • X of length n, Y of length m: D(n,m)

#-------------------------------------------------------------------------------
1.7) Computing_Minimum_Edit_Distance
#-------------------------------------------------------------------------------

- Is a Dynamic programming task.

- ALGORITHM: Levenshtein distance: http://en.wikipedia.org/wiki/Levenshtein_distance

- MinEdit:"1_7.Computing_Minimum_Edit_Distance.mp4" video


#-------------------------------------------------------------------------------
1.8) Backtrace_for_Computing_Alignments
#-------------------------------------------------------------------------------

Note.- 1.8_video = "1_8.Backtrace_for_Computing_Alignments.mp4"

- Goal: in addition to edit distance, we need to align each character of the two
strings to each other.

- How: keeping the trace for each cell of the edit table.
  - MinEdit with backtrace: 1.8_video, t=3:03
  - Algorithm pseudo-code: 1.8_video, t=4:12

- Result = two stings and their alignment.

- Performance: time=O(nm), space==(nm), backtrace=O(n+m)


#-------------------------------------------------------------------------------
1.9) Weighted_Minimum_Edit_Distance
#-------------------------------------------------------------------------------

- Keep a record of the more common errors and use them in the calculation of
  the Edit distance (by adding a weight factor in the distance)

- Algorithm pseudo-code: 1.9_video, t=2:02


#-------------------------------------------------------------------------------
1.10) Minimum_Edit_Distance_in_Computational_Biology
#-------------------------------------------------------------------------------

Note.- 1.10_video = "1_10.Minimum_Edit_Distance_in_Computational_Biology.mp4"

NLP: distance (min) & weights
BIO: similarity (max) & scores

a) ALGORITHM: Needleman-Wunsch:
  http://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm

- Algorithm pseudo-code: 1.10_video, t=2:00

b) The Overlap Detection Variant of the alg. (1.10_video, t=5:00)
  Use only the overlaps of the two sequences ignoring the rest of the matrix.

c) The Local Alignmnet Problem:
  Given two strings, find only the substring whose similarity is maximum.

  c1) Smith-Waterman algorithm:
      http://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm
      - Idea: Ignore badly aligning regions
      - 1.10_video, t= 7:15 - 8:00




################################################################################
## 2) Lecture02.Language Modeling
################################################################################


#-------------------------------------------------------------------------------
# 2.1) Introduction to N-grams (8:41)
#-------------------------------------------------------------------------------

GOAL: predict the next word in a sentence.

a) HOW: assign a probability to a sentence: p(...) > p(...)
- Why?: spell correction, speech recognition, ...
- How detail: apply the Chain Rule of Probability
       P(A, B) = P(A) P(B|A)
       P(x1,x2,x3,…,xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1)
       P(Word1 W2 ..Wn) = MULT(i)  P(Wi/W1 W2 .. Wi-1)

Problem: too many possible sentences to use the chain rule formula
Solution: Simplifying assumption -> the Markov assumption


b) MARKOV ASSUMPTION: use the last few (Wi-k .. Wi-1)) words

   P(Wi / W1 W2 .. Wi-1) =~ P(Wi / Wi-k .. Wi-1)


- Unigram model: use each word separately -> P(w1w2…wn ) =~ MULT(i) P(wi)

- Bigram model: use the previous word -> P(Wi/w1w2…Wi-1 ) =~ P(Wi/Wi-1)

- N-gram models: in general is an insufficient model due to the long distance
  dependencies in sentences, BUT in practice we can use it.


  Hint: N-GRAM MODELS OVER-FITTING (bad generalization) -> n-gram only work
  well for word prediction if the test corpus looks like the training corpus
  (i.e. Both a Shakespeare test (OK), training with Shakes. and test with Wall
  Street Journal (BAD))


#-------------------------------------------------------------------------------
2.2) Estimating N-gram Probabilities (9:38)
#-------------------------------------------------------------------------------

c) Estimating bigram probabilities

  MLE estimate (most likely estimate): maximizes the likelihood of the training
  set T given the model M

  P(Wi / Wi-1) = count(Wi-1, Wi) / count(Wi-1)

(*) Example.-

<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>

Q: p(<s> I am Sam </s>) ?

P(I/<s>) = c(<s>, I) / (c(<s>) = 2/3 = 0.67
P(am/I) = 0.67
P(Sam/am) = 1/2=0.5
P(</s> / Sam) = 0.5

\Warning: chain rule limitation to 2 words (Markov assumption for bigrams)
p(<s> I am Sam </s>) = P(I/<s>) * P(am/I) * P(Sam/am) * P(</s> / Sam) = 1/9

\Warning: Practical issue: we do all in log space (avoid underflow due to low
number multiplications & adding is faster than multiplying)

log(p1*p2*..pn) = log(p1) + log(p2) + .. + log(pn)

log(p(<s> I am Sam </s>)) = log(2/3) + log(2/3) + log(1/2) + log(1/2) = -0.95424
p = 10 exp( -0.95424) = 1/9


(*) More examples: Berkeley Restaurant Project sentences ("L02 Language
Modeling.pdf", page 19)


#-------------------------------------------------------------------------------
   2.3) Evaluation and Perplexity (11:09)
#-------------------------------------------------------------------------------

* Model eval = Applying an evaluation Metric over a Training Set and a Test Set

2.3.1) Extrinsic evaluation Metric

Best evaluation for comparing models A and B
– Put each model in a task (spelling corrector, speech recognizer, ...)
– Run the task, get an accuracy for A and for B
  ● How many misspelled words corrected properly
  ● How many words translated correctly
– Compare accuracy for A and B

PROBLEM: computationally expensive & time consuming (days, weeks...)

2.3.2) Intrinsic evaluation Metric: Perplexity

Quick & dirty solution: bad generalization BUT quick to take a look to a model.

- Intuition of Perplexity: the best language model is one that best predicts an
  unseen test set, thus Gives the highest P(sentence).

- Perplexity = inverse prob. of the test set normalized by the number of words:

	       PP(W) = P(w1w2...wN) exp (-1/N)  # N = number of words 

  LOWER PERPLEXITY = BETTER MODEL -> MIN. PERPLEXITY = MAXIMIZING PROBABILITY


- The Shannon Game intuition for perplexity
• How hard is the task of recognizing digits ‘0,1,2,3,4,5,6,7,8,9’
  Perplexity 10
• How hard is recognizing (30,000) names at Microsoft.
  Perplexity = 30,000

- Perplexity as branching factor


#-------------------------------------------------------------------------------
   2.4) Generalization and Zeros (5:15)
#-------------------------------------------------------------------------------

* Problem: See 2.1) Hint

- Zeros: a type of generalization case that occurs when there were no
occurrences in the training set but exits some occurrences in the test set.

  Bigrams with zero probability -> p(test set)=0 -> perplexity can not be
  calculated (division by 0)
  
#-------------------------------------------------------------------------------
2.5) Add-One (Laplace) smoothing (6:30)
#-------------------------------------------------------------------------------

- Smoothing intuition: avoid the "Zeros generalization problem" by removing a
  bit prob. of the actual found words to give it to the yet unfounded words.

- i.e. Laplace smoothing: pretend we saw each word one more time than we did ->
  just add one to all the counts.

  • MLE estimate: P_MLE (wi | wi-1) = c(wi-1,wi ) / c(wi-1)

  • Add-1 estimate: P_Add-1(wi | wi-1) = c(wi-1,wi )+1 / c(wi-1)+V

(*) Example (Berkeley Restaurant example.- "L02 Language Modeling.pdf", 50) ->
sometimes lead to big changes from the MLE method -> bad for N-grams.

Conclusions:

! So add-1 isn’t used for N-grams:

! But add-1 is used to smooth other NLP models
  – For text classification
  – In domains where the number of zeros isn’t so huge.


#-------------------------------------------------------------------------------
2.6) Interpolation, Backoff, and Web-Scale LMs (10:25)
#-------------------------------------------------------------------------------

2.6.1) Interpolation

Intuition: Sometimes it helps to use less context (when you haven’t learned
much about the context)

a) Backoff:
  – use trigram if you have good evidence,
  – otherwise bigram, otherwise unigram

b) Interpolation:
   – mix unigram, bigram, trigram

   b1) e.g. Linear Interpolation

       P(Wn|Wn-1Wn-2) = λ1*P(Wn|Wn-1Wn-2)     # λ1 trigram
       		        + λ2*P(Wn|Wn-1)       # λ2 bigram
       		        + λ3*P(Wn)	      # λ3 monogram

   b2) How to set the lambdas.- split the data in 3 sets (idem machine learning)
       - Training set: Fix the N-gram probabilities
       - Dev. set: search for λs that give largest probability to dev. set
       - Test set: check how good is the model.

!! EXPERIMENTALLY CHECKED: Interpolation works better than backoff


2.6.2) Unknown words: Open versus closed vocabulary tasks

If we know all the words in advanced
   – Vocabulary V is fixed
   – Closed vocabulary task

Otherwise:
step 1) Create an unknown word token <UNK>
step 2) Training of <UNK> probabilities
 a) Create a fixed lexicon L of size V (may be by removing less probable words)
 b) At text normalization phase, any training word not in L changed to <UNK>
 c) Now we train its probabilities like a normal word
 d) At decoding time:
    If text input: Use UNK probabilities for any word not in training set.


#-------------------------------------------------------------------------------
2.7) Good-Turing Smoothing (15:35)
#-------------------------------------------------------------------------------

- Intuition: use the count of things we’ve seen once to help estimate the count
  of things we’ve never seen.

- Notation: Nc = Frequency of frequency c = count of things we've seen c times.

- Good-Turing smoothing intuition (fishing example):
  You are fishing (a scenario from Josh Goodman), and caught: 10 carp, 3 perch,
  2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish:
  Then, N1=3,  N2=1, N3=1, N4=..=N9=0, N10=1

  Q1) How likely is it that next species is trout?  1/18

  Q2) How likely is it that next species is new (i.e. catfish or bass)
  – N1=3 -> using our estimate of things-we-saw-once to estimate the new things
         -> Q2 = 3/18 (because N1=3)
  
  Q3) Assuming so, how likely is it that next species is trout?
      Given Q2, we know that must be less than 1/18 (at Q1)
 
- Good Turing calculations:

  -- freq 0 (unseen)
     - P*GT(unseen) = N1 / N

  -- freq C (seen C-times)
     - C* = (C+1)*(Nc+1) / Nc
     - P*GT(C*) = (C*)/N

  In our example (Q3):
  Unseen: P*GT(unseen) = N1/N = 3/18
  Seen once (trout):
    C*(trout) = (1+1)*N2 / N1 = 2/3 
    P*GT(trout) = C*(trout)/N = (2/3)/18 = 1/27   	 


2.7.2) Ney et al. Good Turing Intuition:

       "L02 Language Modeling.pdf", 71-72)

2.7.3) Good-Turing complications:

       "L02 Language Modeling.pdf", 73)


#-------------------------------------------------------------------------------
2.8) Kneser-Ney Smoothing (8:59)
#-------------------------------------------------------------------------------

-Definition [http://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing]

Kneser–Ney smoothing is a method primarily used to calculate the probability
distribution of n-grams in a document based on their histories.

[1] It is widely considered the most effective method of smoothing due to its
use of absolute discounting by subtracting a fixed value from the probability's
lower order terms to omit n-grams with lower frequencies.

This approach has been considered equally effective for both higher and lower
order n-grams.

- Equations (bigram & n-gram)

  "L02 Language Modeling.pdf", 81-82)
  http://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing

- Method explanation: "L02 Language Modeling.pdf", 76-82)


2.8.end) N-gram Smoothing Summary

• Add-1 smoothing: OK for text categorization, not for language modeling

• The most commonly used method: Extended Interpolated Kneser-Ney

• For very large N-grams like the Web: Stupid backoff (No discounting, just use
  relative frequencies)


################################################################################
## 3) Lecture03.Spelling_Correction
################################################################################


#-------------------------------------------------------------------------------
# 3.1) The Spelling Correction Task (5:39)
#-------------------------------------------------------------------------------

a) Tasks:
• Spelling Error Detection
• Spelling Error Correction
  – Auto-correct (hte -> the)
  – Suggest a correction (list)

b) Spelling errors in a text are either "non-word errors" or "real-word errors."

   - non-word errors: produces a string of letters that is not a word, as when
     "the" is typed as "teh."

   - real-word errors
     • Typographical errors: three -> there      
     • Cognitive errors (homophones): too -> two


#-------------------------------------------------------------------------------
# 3.2) The Noisy Channel Model of Spelling (19:30)
#-------------------------------------------------------------------------------

3.2.1) Noisy Channel Intuition

       ERROR:       original word  --> NOISY CHANNEL --> noisy word (X)

       SOLUTION:    noisy word  --> DECODER --> guessed word (W)


3.2.2) Noisy Channel equation

       w' = argmax P(w | x)  =   (Bayes Rule)
            w in V

       w' = argmax (P(x | w) P(w)) / P(x) = (maximizing -> P(x)->0)
            w in V

       w' = argmax (P(x | w) P(w))
            w in V

	    - P(x|w): Channel/Error Modeling Term
	    - P(w): Language Model Term


3.2.3) Spelling Error Correction: suggest a correction word using the Nosy
       channel model (see example at "L03 Spelling Correction.pdf", 14-30)

       Resolve: w' = argmax (P(x | w) P(w))

step_1: Candidate generation (finding W)

	- Method: Words with similar pronunciation
	- Method: Words with similar spelling
	  • Damerau-Levenshtein edit distance (Insertion, Deletion ,
            Substitution, and Transposition of two adjacent letters)

step_2: Calculate the Language Model Term (P(w))

	- Use any of the language modeling algorithms we’ve learned (unigram,
          ..., Backoff)

step_3: Calculate the w): Channel Modeling Term P(x|w)

	- P(x|w) = probability of the edit
          	   (deletion/insertion/substitution/transposition)

a) Generate the confusion matrix [3.1.4], [3.1.5]
b) Use the matrix to calculate a list of P(x|w)

step_4: Calculate the Noisy channel probability: w' = (P(x | w) P(w))

step_5: Choose the candidate word (w') with the greater prob.

step_6: Evaluation using spelling error test sets [3.1]


#-------------------------------------------------------------------------------
# 3.3) Real-Word Spelling Correction (9:19)
#-------------------------------------------------------------------------------

step_1: For each word in sentence Generate candidate set
	  ● the word itself
	  ● all single-letter edits that are English words
	  ● words that are homophones

step_2: Choose best candidates
	● Noisy channel model
	● Task-specific classifier

step_3: Noisy channel for real-word spell correction
a) Given a sentence w1,w2,w3,…,wn generate a set of candidates for each word wi
  Candidate(w1) = {w1, w’1 , w’’1 , w’’’1 ,…}
  Candidate(w2) = {w2, w’2 , w’’2 , w’’’2 ,…}
  Candidate(wn) = {wn, w’n , w’’n , w’’’n ,…}
b) Choose the sequence W that maximizes P(W)

  SIMPLIFICATION: in practice, fix to one error per sentence and them Choose
  the sequence W that maximizes P(W)

Where to get the probabilities
• Language model: 
• Channel model
  – Same as for non-word spelling correction
  – Plus need probability for no error, P(w|w)


#-------------------------------------------------------------------------------
# 3.4) State of the Art Systems (7:10)
#-------------------------------------------------------------------------------

- HCI issues in spelling

• If very confident in correction ->  Autocorrect
• Less confident -> Give the best correction
• Less confident -> Give a correction list
• Unconfident -> Just flag as an error


- State of the art noisy channel
   
   In the ecuation at 3.2.2) in practice weigh the probabilies (λ)

       w' = argmax P(x|w) (P(w)^λ)
            w in V

   and learn λ from a development test set.


- Phonetic error model: use pronunciation, convert misspelling to metaphone
  pronunciation.



############################################################################
#
# References
#
############################################################################


[2.2] Estimating N-gram Probabilities (9:38)

[2.2.1] Language Modeling Toolkits
	SRILM – http://www.speech.sri.com/projects/srilm/

[2.2.2] Google Book N-grams - http://ngrams.googlelabs.com/

[3.1] Some spelling error test sets
[3.1.1] Wikipedia’s list of common English misspelling - http://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines
[3.1.2] Aspell filtered version of that list - http://aspell.net/test/
[3.1.3] Birkbeck spelling err corpus - http://www.ota.ox.ac.uk/headers/0643.xml
[3.1.4] Peter Norvig list of errors - http://norvig.com/ngrams/spell-errors.txt
[3.1.5] Peter Norvig’s list of counts of single-edit errors - http://norvig.com/ngrams/spell-errors.txt


