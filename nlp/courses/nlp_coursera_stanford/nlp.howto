################################################################################
# INDEX
################################################################################

1) Lecture01_1.Basic_Text_Processing 
   1.1) Regular Expressions
   1.2) Regular_Expressions_in_Practical_NLP
   1.3) Word Tokenization
   	a) Simple Tokenization in Unix
   1.4) Word Normalization and Stemming
   	a) Lemmatization
   1.5) Sentence Segmentation
1_2) Lecture01.2: Edit_Distance
   1.6) Defining_Minimum_Edit_Distance
   1.7) Computing_Minimum_Edit_Distance
   1.8) Backtrace_for_Computing_Alignments
   1.9) Weighted_Minimum_Edit_Distance
   1.10) Minimum_Edit_Distance_in_Computational_Biology

2) Lecture02.Language Modeling
   2.1) Introduction to N-grams (8:41)
   2.2) Estimating N-gram Probabilities (9:38)
   2.3) Evaluation and Perplexity (11:09)
   2.4) Generalization and Zeros (5:15)
   2.5) Add-One (Laplace) smoothing (6:30)
   2.6) Interpolation, Backoff, and Web-Scale LMs (10:25)
   	2.6.1) Interpolation
   	2.6.2) Unknown words: Open versus closed vocabulary tasks
   2.7) Good-Turing Smoothing (15:35)   
   	2.7.2) Ney et al. Good Turing Intuition:
	2.7.3) Good-Turing complications:
   2.8) Kneser-Ney Smoothing (8:59)
   	2.8.end) N-gram Smoothing Summary
	
   Recap
   ● N-gram Language Model
   ● Estimation
   ● Smoothing
      – Laplace
      – Good-Turing
      – Kneser-Ney
   

3) Lecture03.Spelling_Correction
   3.1) The Spelling Correction Task (5:39)
   3.2) The Noisy Channel Model of Spelling (19:30)
   3.3) Real-Word Spelling Correction (9:19)
   3.4) State of the Art Systems (7:10)


4) Lecture 04: Text Classification
   4.1) The Task of Text Classification
   4.2) Naive Bayes Intuition
   4.3) Formalizing the Naive Bayes Classifier
   4.4) Naive Bayes: Learning
   4.5) Naive Bayes: Relationship to Language Modeling
   4.6) Multinomial Naive Bayes: A Worked Example
      4.6.1) Example
      4.6.2) Naive Bayes Summary
   4.7) Precision, Recall, and the F measure
   4.8) Text Classification: Evaluation
   4.9) Text Classification: Practical Issues


5) Sentiment Analysis
   5.1) What is Sentiment Analysis? (7:17)
   5.2) Sentiment Analysis: A baseline algorithm (13:27)
   5.3) Sentiment Lexicons (8:37)
   5.4) Learning Sentiment Lexicons (14:45)
   5.5) Other Sentiment Tasks (11:01)


Annex) References



################################################################################
Lecture01_1.Basic_Text_Processing 
################################################################################

#-------------------------------------------------------------------------------
# 1.3) Word Tokenization
#-------------------------------------------------------------------------------

• Type: an element of the vocabulary.
• Token: an instance of that type in running text.
  V = vocabulary = set of types (|V| = size)
  N = number of tokens

# Simple Tokenization in Unix (get V)
tr 'A-Z' 'a-z' <materials/shakes.txt  # to low-case
   | tr -sc 'A-Za-z' '\n'             # remove spaces & print one token per line
   | sort                             # sort alphabet.
   | uniq -c                          # merge & count each 'Type' (V)
   | sort -n -r | less                # sort by frequency


Issues is tokenization:
• I'm, isn't
• state-of-the-art -> state of the art ?
• lowercase -> lower-case, lowercase, lower case ?
• Languages issues: Chinese & Japanese has no spaces between words, ...


#-------------------------------------------------------------------------------
1.4) Word Normalization and Stemming
#-------------------------------------------------------------------------------

a) Lemmatization: reduce inflections or variant forms to base form.
   • am, are, is -> be
   • car, cars, car's -> car

b) Stemming

* Morphemes: stems (core) + affixes (prefix & suffix)
   
* Stemming = reduce terms to their stems in info. retrieval = chopping affixes.
  – language dependent
  – e.g., automate(s), automatic, automation -> all reduced to automat.
  - e.g. (English) Porter's algorithm.


#-------------------------------------------------------------------------------
1.5) Sentence Segmentation
#-------------------------------------------------------------------------------

Find the final of sentences (?, !, .): use binary classifiers (decision trees).



################################################################################
Lecture01_1.Basic_Text_Processing 
################################################################################

#-------------------------------------------------------------------------------
# 1.6) Defining_Minimum_Edit_Distance
#-------------------------------------------------------------------------------

def) Minimum number of editing operations (insertion, deletion, substitution),
needed to transform one into the other.

       • X of length n, Y of length m: D(n,m)

#-------------------------------------------------------------------------------
1.7) Computing_Minimum_Edit_Distance
#-------------------------------------------------------------------------------

- Is a Dynamic programming task.

- ALGORITHM: Levenshtein distance: http://en.wikipedia.org/wiki/Levenshtein_distance

- MinEdit:"1_7.Computing_Minimum_Edit_Distance.mp4" video


#-------------------------------------------------------------------------------
1.8) Backtrace_for_Computing_Alignments
#-------------------------------------------------------------------------------

Note.- 1.8_video = "1_8.Backtrace_for_Computing_Alignments.mp4"

- Goal: in addition to edit distance, we need to align each character of the two
strings to each other.

- How: keeping the trace for each cell of the edit table.
  - MinEdit with backtrace: 1.8_video, t=3:03
  - Algorithm pseudo-code: 1.8_video, t=4:12

- Result = two stings and their alignment.

- Performance: time=O(nm), space==(nm), backtrace=O(n+m)


#-------------------------------------------------------------------------------
1.9) Weighted_Minimum_Edit_Distance
#-------------------------------------------------------------------------------

- Keep a record of the more common errors and use them in the calculation of
  the Edit distance (by adding a weight factor in the distance)

- Algorithm pseudo-code: 1.9_video, t=2:02


#-------------------------------------------------------------------------------
1.10) Minimum_Edit_Distance_in_Computational_Biology
#-------------------------------------------------------------------------------

Note.- 1.10_video = "1_10.Minimum_Edit_Distance_in_Computational_Biology.mp4"

NLP: distance (min) & weights
BIO: similarity (max) & scores

a) ALGORITHM: Needleman-Wunsch:
  http://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm

- Algorithm pseudo-code: 1.10_video, t=2:00

b) The Overlap Detection Variant of the alg. (1.10_video, t=5:00)
  Use only the overlaps of the two sequences ignoring the rest of the matrix.

c) The Local Alignmnet Problem:
  Given two strings, find only the substring whose similarity is maximum.

  c1) Smith-Waterman algorithm:
      http://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm
      - Idea: Ignore badly aligning regions
      - 1.10_video, t= 7:15 - 8:00




################################################################################
## 2) Lecture02.Language Modeling
################################################################################


#-------------------------------------------------------------------------------
# 2.1) Introduction to N-grams (8:41)
#-------------------------------------------------------------------------------

GOAL: predict the next word in a sentence.

a) HOW: assign a probability to a sentence: p(...) > p(...)
- Why?: spell correction, speech recognition, ...
- How detail: apply the Chain Rule of Probability
       P(A, B) = P(A) P(B|A)
       P(x1,x2,x3,…,xn) = P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1)
       P(Word1 W2 ..Wn) = MULT(i)  P(Wi/W1 W2 .. Wi-1)

Problem: too many possible sentences to use the chain rule formula
Solution: Simplifying assumption -> the Markov assumption


b) MARKOV ASSUMPTION: use the last few (Wi-k .. Wi-1)) words

   P(Wi / W1 W2 .. Wi-1) =~ P(Wi / Wi-k .. Wi-1)


- Unigram model: use each word separately -> P(w1w2…wn ) =~ MULT(i) P(wi)

- Bigram model: use the previous word -> P(Wi/w1w2…Wi-1 ) =~ P(Wi/Wi-1)

- N-gram models: in general is an insufficient model due to the long distance
  dependencies in sentences, BUT in practice we can use it.


  Hint: N-GRAM MODELS OVER-FITTING (bad generalization) -> n-gram only work
  well for word prediction if the test corpus looks like the training corpus
  (i.e. Both a Shakespeare test (OK), training with Shakes. and test with Wall
  Street Journal (BAD))


#-------------------------------------------------------------------------------
2.2) Estimating N-gram Probabilities (9:38)
#-------------------------------------------------------------------------------

c) Estimating bigram probabilities

  MLE estimate (most likely estimate): maximizes the likelihood of the training
  set T given the model M

  P(Wi / Wi-1) = count(Wi-1, Wi) / count(Wi-1)

(*) Example.-

<s> I am Sam </s>
<s> Sam I am </s>
<s> I do not like green eggs and ham </s>

Q: p(<s> I am Sam </s>) ?

P(I/<s>) = c(<s>, I) / (c(<s>) = 2/3 = 0.67
P(am/I) = 0.67
P(Sam/am) = 1/2=0.5
P(</s> / Sam) = 0.5

\Warning: chain rule limitation to 2 words (Markov assumption for bigrams)
p(<s> I am Sam </s>) = P(I/<s>) * P(am/I) * P(Sam/am) * P(</s> / Sam) = 1/9

\Warning: Practical issue: we do all in log space (avoid underflow due to low
number multiplications & adding is faster than multiplying)

log(p1*p2*..pn) = log(p1) + log(p2) + .. + log(pn)

log(p(<s> I am Sam </s>)) = log(2/3) + log(2/3) + log(1/2) + log(1/2) = -0.95424
p = 10 exp( -0.95424) = 1/9


(*) More examples: Berkeley Restaurant Project sentences ("L02 Language
Modeling.pdf", page 19)


#-------------------------------------------------------------------------------
   2.3) Evaluation and Perplexity (11:09)
#-------------------------------------------------------------------------------

* Model eval = Applying an evaluation Metric over a Training Set and a Test Set

2.3.1) Extrinsic evaluation Metric

Best evaluation for comparing models A and B
– Put each model in a task (spelling corrector, speech recognizer, ...)
– Run the task, get an accuracy for A and for B
  ● How many misspelled words corrected properly
  ● How many words translated correctly
– Compare accuracy for A and B

PROBLEM: computationally expensive & time consuming (days, weeks...)

2.3.2) Intrinsic evaluation Metric: Perplexity

Quick & dirty solution: bad generalization BUT quick to take a look to a model.

- Intuition of Perplexity: the best language model is one that best predicts an
  unseen test set, thus Gives the highest P(sentence).

- Perplexity = inverse prob. of the test set normalized by the number of words:

	       PP(W) = P(w1w2...wN) exp (-1/N)  # N = number of words 

  LOWER PERPLEXITY = BETTER MODEL -> MIN. PERPLEXITY = MAXIMIZING PROBABILITY


- The Shannon Game intuition for perplexity
  • How hard is the task of recognizing digits ‘0,1,2,3,4,5,6,7,8,9’
    Perplexity 10
    • How hard is recognizing (30,000) names at Microsoft.
    Perplexity = 30,000

- Perplexity as branching factor


#-------------------------------------------------------------------------------
   2.4) Generalization and Zeros (5:15)
#-------------------------------------------------------------------------------

* Problem: See 2.1) Hint

- Zeros: a type of generalization case that occurs when there were no
occurrences in the training set but exits some occurrences in the test set.

  Bigrams with zero probability -> p(test set)=0 -> perplexity can not be
  calculated (division by 0)
  
#-------------------------------------------------------------------------------
2.5) Add-One (Laplace) smoothing (6:30)
#-------------------------------------------------------------------------------

- Smoothing intuition: avoid the "Zeros generalization problem" by removing a
  bit prob. of the actual found words to give it to the yet unfounded words.

- i.e. Laplace smoothing: pretend we saw each word one more time than we did ->
  just add one to all the counts.

  • MLE estimate: P_MLE (wi | wi-1) = c(wi-1,wi ) / c(wi-1)

  • Add-1 estimate: P_Add-1(wi | wi-1) = c(wi-1,wi )+1 / c(wi-1)+V

(*) Example (Berkeley Restaurant example.- "L02 Language Modeling.pdf", 50) ->
sometimes lead to big changes from the MLE method -> bad for N-grams.

Conclusions:

! So add-1 isn’t used for N-grams:

! But add-1 is used to smooth other NLP models
  – For text classification
  – In domains where the number of zeros isn’t so huge.


#-------------------------------------------------------------------------------
2.6) Interpolation, Backoff, and Web-Scale LMs (10:25)
#-------------------------------------------------------------------------------

2.6.1) Interpolation

Intuition: Sometimes it helps to use less context (when you haven’t learned
much about the context)

a) Backoff:
  – use trigram if you have good evidence,
  – otherwise bigram, otherwise unigram

b) Interpolation:
   – mix unigram, bigram, trigram

   b1) e.g. Linear Interpolation

       P(Wn|Wn-1Wn-2) = λ1*P(Wn|Wn-1Wn-2)     # λ1 trigram
       		        + λ2*P(Wn|Wn-1)       # λ2 bigram
       		        + λ3*P(Wn)	      # λ3 monogram

   b2) How to set the lambdas.- split the data in 3 sets (idem machine learning)
       - Training set: Fix the N-gram probabilities
       - Dev. set: search for λs that give largest probability to dev. set
       - Test set: check how good is the model.

!! EXPERIMENTALLY CHECKED: Interpolation works better than backoff


2.6.2) Unknown words: Open versus closed vocabulary tasks

If we know all the words in advanced
   – Vocabulary V is fixed
   – Closed vocabulary task

Otherwise:
step 1) Create an unknown word token <UNK>
step 2) Training of <UNK> probabilities
 a) Create a fixed lexicon L of size V (may be by removing less probable words)
 b) At text normalization phase, any training word not in L changed to <UNK>
 c) Now we train its probabilities like a normal word
 d) At decoding time:
    If text input: Use UNK probabilities for any word not in training set.


#-------------------------------------------------------------------------------
2.7) Good-Turing Smoothing (15:35)
#-------------------------------------------------------------------------------

- Intuition: use the count of things we’ve seen once to help estimate the count
  of things we’ve never seen.

- Notation: Nc = Frequency of frequency c = count of things we've seen c times.

- Good-Turing smoothing intuition (fishing example):
  You are fishing (a scenario from Josh Goodman), and caught: 10 carp, 3 perch,
  2 whitefish, 1 trout, 1 salmon, 1 eel = 18 fish:
  Then, N1=3,  N2=1, N3=1, N4=..=N9=0, N10=1

  Q1) How likely is it that next species is trout?  1/18

  Q2) How likely is it that next species is new (i.e. catfish or bass)
  – N1=3 -> using our estimate of things-we-saw-once to estimate the new things
         -> Q2 = 3/18 (because N1=3)
  
  Q3) Assuming so, how likely is it that next species is trout?
      Given Q2, we know that must be less than 1/18 (at Q1)
 
- Good Turing calculations:

  -- freq 0 (unseen)
     - P*GT(unseen) = N1 / N

  -- freq C (seen C-times)
     - C* = (C+1)*(Nc+1) / Nc
     - P*GT(C*) = (C*)/N

  In our example (Q3):
  Unseen: P*GT(unseen) = N1/N = 3/18
  Seen once (trout):
    C*(trout) = (1+1)*N2 / N1 = 2/3 
    P*GT(trout) = C*(trout)/N = (2/3)/18 = 1/27   	 


2.7.2) Ney et al. Good Turing Intuition:

       "L02 Language Modeling.pdf", 71-72)

2.7.3) Good-Turing complications:

       "L02 Language Modeling.pdf", 73)


#-------------------------------------------------------------------------------
2.8) Kneser-Ney Smoothing (8:59)
#-------------------------------------------------------------------------------

-Definition [http://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing]

Kneser–Ney smoothing is a method primarily used to calculate the probability
distribution of n-grams in a document based on their histories.

[1] It is widely considered the most effective method of smoothing due to its
use of absolute discounting by subtracting a fixed value from the probability's
lower order terms to omit n-grams with lower frequencies.

This approach has been considered equally effective for both higher and lower
order n-grams.

- Equations (bigram & n-gram)

  "L02 Language Modeling.pdf", 81-82)
  http://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing

- Method explanation: "L02 Language Modeling.pdf", 76-82)


2.8.end) N-gram Smoothing Summary

• Add-1 smoothing: OK for text categorization, not for language modeling

• The most commonly used method: Extended Interpolated Kneser-Ney

• For very large N-grams like the Web: Stupid backoff (No discounting, just use
  relative frequencies)



################################################################################
## 3) Lecture03.Spelling_Correction
################################################################################


#-------------------------------------------------------------------------------
# 3.1) The Spelling Correction Task (5:39)
#-------------------------------------------------------------------------------

a) Tasks:
• Spelling Error Detection
• Spelling Error Correction
  – Auto-correct (hte -> the)
  – Suggest a correction (list)

b) Spelling errors in a text are either "non-word errors" or "real-word errors."

   - non-word errors: produces a string of letters that is not a word, as when
     "the" is typed as "teh."

   - real-word errors
     • Typographical errors: three -> there      
     • Cognitive errors (homophones): too -> two


#-------------------------------------------------------------------------------
# 3.2) The Noisy Channel Model of Spelling (19:30)
#-------------------------------------------------------------------------------

3.2.1) Noisy Channel Intuition

       ERROR:       original word  --> NOISY CHANNEL --> noisy word (X)

       SOLUTION:    noisy word  --> DECODER --> guessed word (W)


3.2.2) Noisy Channel equation

       w' = argmax P(w | x)  =   (Bayes Rule)
            w in V

       w' = argmax (P(x | w) P(w)) / P(x) = (maximizing -> P(x)->0)
            w in V

       w' = argmax (P(x | w) P(w))
            w in V

	    - P(x|w): Channel/Error Modeling Term
	    - P(w): Language Model Term


3.2.3) Spelling Error Correction: suggest a correction word using the Nosy
       channel model (see example at "L03 Spelling Correction.pdf", 14-30)

       Resolve: w' = argmax (P(x | w) P(w))

step_1: Candidate generation (finding W)

	- Method: Words with similar pronunciation
	- Method: Words with similar spelling
	  • Damerau-Levenshtein edit distance (Insertion, Deletion ,
            Substitution, and Transposition of two adjacent letters)

step_2: Calculate the Language Model Term (P(w))

	- Use any of the language modeling algorithms we’ve learned (unigram,
          ..., Backoff)

step_3: Calculate the w): Channel Modeling Term P(x|w)

	- P(x|w) = probability of the edit
          	   (deletion/insertion/substitution/transposition)

a) Generate the confusion matrix [3.1.4], [3.1.5]
b) Use the matrix to calculate a list of P(x|w)

step_4: Calculate the Noisy channel probability: w' = (P(x | w) P(w))

step_5: Choose the candidate word (w') with the greater prob.

step_6: Evaluation using spelling error test sets [3.1]


#-------------------------------------------------------------------------------
# 3.3) Real-Word Spelling Correction (9:19)
#-------------------------------------------------------------------------------

step_1: For each word in sentence Generate candidate set
	  ● the word itself
	  ● all single-letter edits that are English words
	  ● words that are homophones

step_2: Choose best candidates
	● Noisy channel model
	● Task-specific classifier

step_3: Noisy channel for real-word spell correction
a) Given a sentence w1,w2,w3,…,wn generate a set of candidates for each word wi
  Candidate(w1) = {w1, w’1 , w’’1 , w’’’1 ,…}
  Candidate(w2) = {w2, w’2 , w’’2 , w’’’2 ,…}
  Candidate(wn) = {wn, w’n , w’’n , w’’’n ,…}
b) Choose the sequence W that maximizes P(W)

  SIMPLIFICATION: in practice, fix to one error per sentence and them Choose
  the sequence W that maximizes P(W)

Where to get the probabilities
• Language model: 
• Channel model
  – Same as for non-word spelling correction
  – Plus need probability for no error, P(w|w)


#-------------------------------------------------------------------------------
# 3.4) State of the Art Systems (7:10)
#-------------------------------------------------------------------------------

- HCI issues in spelling

• If very confident in correction ->  Autocorrect
• Less confident -> Give the best correction
• Less confident -> Give a correction list
• Unconfident -> Just flag as an error


- State of the art noisy channel
   
   In the ecuation at 3.2.2) in practice weigh the probabilies (λ)

       w' = argmax P(x|w) (P(w)^λ)
            w in V

   and learn λ from a development test set.


- Phonetic error model: use pronunciation, convert misspelling to metaphone
  pronunciation.



################################################################################
## 4) Lecture 04: Text Classification
################################################################################


#-------------------------------------------------------------------------------
# 4.1) The Task of Text Classification
#-------------------------------------------------------------------------------

- Resolve problems like: Spam detection; Authorship, Age/gender, language
  identification, Sentiment analysis

- Text Classification: definition
  • Input:
    – a document d
    – a fixed set of classes C = {c1, c2,…, cJ}
  • Output: a predicted class c in C

- Classification Methods: Hand-coded rules / Supervised Machine Learning

a) Hand-coded rules
Rules based on combinations of words or other features
GOOD: Accuracy can be high if rules carefully refined by expert
BAD: building and maintaining these rules is expensive

b) Supervised Machine Learning
• Input:
  – a document d
  – a fixed set of classes C = {c1, c2,…, cJ}
  – A training set of m hand-labeled documents (d1,c1),...., (dm,cm)
• Output:
  – a learned classifier γ:d in c


#-------------------------------------------------------------------------------
# 4.2) Naive Bayes Intuition
#-------------------------------------------------------------------------------

- "Bag of words" = all the document OR a subset of words

       Gamma(Bag) = C ;   class C: e.g. positive or negative class

- WARNING: categories classes: they include words defining the class
  (e.g. "excellent", "brilliant", ... for a positive review class).
  There are several list in the Web [5.3.1], [5.3.2].
         

#-------------------------------------------------------------------------------
# 4.3) Formalizing the Naive Bayes Classifier
#-------------------------------------------------------------------------------

For a document d and a class c

    C_map = argmax P(c | d)  = argmax P(d | c)P(c)
    	    c in C             c in C

- Document d represented as features x1..xn

    C_map = argmax P(x1, x2,…, xn | c)P(c)
            c in C


a) Multinomial Naive Bayes Classifier

   C_sub_NB = argmax  {P(Cj)  MULT  P(x|c)}
              c in C         x in X

b) Applying Multinomial Naive Bayes Classifiers to Text Classification:

- positions <- all word positions in test document

   C_sub_NB = argmax  {P(Cj)     MULT        P(Xi|Cj)}
              Cj in C        i in positions


e.g.- class c=china; x1=Sanghai, x2=and, x3=Shenzhen, ...


#-------------------------------------------------------------------------------
# 4.4) Naive Bayes: Learning
#-------------------------------------------------------------------------------

Equations (with Add-One (Laplace) smoothing)

Prior:  P(c) = Nc / N
	# prob. class C = docs of class C / all docs


Likelihood: P(Wi/c) = count(Wi/c)+1 / count(w/c)+[V]
# prob. word-i in class C =count Wi in the class / ALL the words in the class


P(class / document):   P(d|c) = MULT P(word|class) / P(d)  (* see 4.5)


#-------------------------------------------------------------------------------
# 4.5) Naive Bayes: Relationship to Language Modeling
#-------------------------------------------------------------------------------

• Naive Bayes classifiers can use any sort of features: URL, email address,
  dictionaries, network features

• But IF, we use only word features then we use all of the words in the text
  (not a subset) THEN Naive Bayes =~ language modeling:

  Each class = a unigram language model

  thus, assigning each sentence: P(s|c)= MULT P(word|c)


#-------------------------------------------------------------------------------
# 4.6) Multinomial Naive Bayes: A Worked Example
#-------------------------------------------------------------------------------

4.6.1) Example.- see detail "4_6.Multinomial_Naive_Bayes.A_Worked_Example.mp4"

4.6.2) Naive Bayes Summary: 

• Very Fast, low storage requirements
 • Robust to Irrelevant Features
 • Very good in domains with many equally important features
 • Optimal if the independence assumptions hold: If assumed independence is
 correct, then it is the Bayes Optimal Classifier for problem
 • A good dependable baseline for text classification

– But we will see other classifiers that give better accuracy


#-------------------------------------------------------------------------------
# 4.7) Precision, Recall, and the F measure
#-------------------------------------------------------------------------------

- The 2-by-2 contingency table

                     | CORRECT | NOT CORRECT |
		     -------------------------
PREDICTED (seleted)  |   TP    |      FP     |
		     -------------------------
      (not seleted)  |   FN    |      TN     |
		     -------------------------

* Bad measure for Skewed (uncommon classes): ACCURACY

  - accuracy = (TP+TN)/(TP+TN+FP+FN)

* Good measure for Skewed: PRECISION, RECALL, F1SCORE

  - precision = TP / TP+FP    # % of the selected that are correct values

  - recall = TP / TP+ FN      # % of correct values that are selected

  WARNING: there is a trade-off between precision and recall -> F1SCORE

  - F1 = 2PR/(P+R)


#-------------------------------------------------------------------------------
# 4.8) Text Classification: Evaluation
#-------------------------------------------------------------------------------

- Documents for make an evaluation: Classic Reuters-21578 Data Set

a) Create the confusion matrix c: For each pair of classes <c1,c2> how many
documents from c1 were incorrectly assigned to c2?

b) Per class evaluation measures over the confusion matrix: recall, precision,
accuracy.

c) Micro- vs. Macro-Averaging:
• Macroaveraging: Compute performance for each class, then average.
• Microaveraging: Collect decisions for all classes, compute contingency table,
  evaluate.

d) Development Test Sets and Cross-validation: training+CV+Test sets


#-------------------------------------------------------------------------------
# 4.9) Text Classification: Practical Issues
#-------------------------------------------------------------------------------

4.9.1) Selecting the Classifier

a) No training data?

   Use Manually written rules

   "If (wheat or grain) and not (whole or bread) then Categorize as grain"

• Need careful crafting
  – Human tuning on development data
  – Time-consuming: 2 days per class


b) Very little data?

• Use Naïve Bayes
  – Naïve Bayes is a “high-bias” algorithm (low overfitting).
  
• Get more labeled data
  – Find clever ways to get humans to label data for you

• Try semi-supervised training methods:
  – Bootstrapping, EM over unlabeled documents, …


c) A reasonable amount of data?

• Perfect for all the clever classifiers
  – SVM
  – Regularized Logistic Regression

• You can even use user-interpretable decision trees
  – Users like to hack
  – Management likes quick fixes


d) A huge amount of data?

• Can achieve high accuracy!
• At a cost:
  – SVMs (train time) or kNN (test time) can be too slow
  – Regularized logistic regression can be somewhat better
• So Naïve Bayes can come back into its own again!

WARNING: With enough data the Classifier may not matter (similar results)


4.9.2) Real-world systems generally combine:
• Automatic classification
• Manual review of uncertain/difficult/"new” cases

4.9.3) Underflow Prevention: use the "log space" -> Model = MAX(sum of weights)

4.9.4) How to tweak performance

• Domain-specific features and weights: very important in real performance

• Sometimes need to collapse terms:
  – Part numbers, chemical formulas, …
  – But stemming generally doesn’t help

• Upweighting: Counting a word as if it occurred twice:
  – title words (Cohen & Singer 1996)
  – first sentence of each paragraph (Murata, 1999)
  – In sentences that contain title words (Ko et al, 2002)




################################################################################
## 5) Sentiment Analysis
################################################################################


#-------------------------------------------------------------------------------
# 5.1) What is Sentiment Analysis? (7:17)
#-------------------------------------------------------------------------------

• Sentiment analysis is the detection of attitudes “enduring, affectively
  colored beliefs, dispositions towards objects or persons”

  1. Holder (source) of attitude
  2. Target (aspect) of attitude
  3. Type of attitude
     • From a set of types (Like, love, hate, value, desire, etc.)
     • Or (more commonly) simple weighted polarity: (positive, negative, ...)
  4. Text containing the attitude: sentence or entire document


• Simplest task: Is the attitude of this text positive or negative?

• More complex: Rank the attitude of this text from 1 to 5

• Advanced: Detect the target, source, or complex attitude types


#-------------------------------------------------------------------------------
# 5.2) Sentiment Analysis: A baseline algorithm (13:27)
#-------------------------------------------------------------------------------

5.2.1) Binarized (Boolean feature) Multinomial Naive Bayes

       a) First remove all duplicate words from d
       b) Then compute NB using the same equation: (4.3.b)

5.2.2) Issues

a) Tokenization Issues (Twitter, XML, phone numbers, ...)
b) Handle negation
c) Other classifiers: MaxEnt and SVM tend to do better than Naive Bayes.
d) Which words to use: ALL (5.2.1) vs some of them.


#-------------------------------------------------------------------------------
# 5.3) Sentiment Lexicons (8:37)
#-------------------------------------------------------------------------------

• There are several sites that provides lexicons as Class indexes [5.3.1],
  [5.3.2], (see "L05 Sentiment Analysis.pdf" , 36-41)

• polarity lexicons: How likely is each word to appear in each sentiment class?
  - raw counts -> BAD
  - likelihood -> OK
  - Scaled likelihood: Make them comparable between words: P(w/c) / P(w)
    See example "Analyzing the polarity of each word in IMDB" at "L05 Sentiment
    Analysis.pdf", page 43.

• Logical negation: Is logical negation (no, not) associated with negative
  sentiment?


#-------------------------------------------------------------------------------
# 5.4) Learning Sentiment Lexicons (14:45)
#-------------------------------------------------------------------------------

* How to build our own Sentiment Lexicon? ...

... Use Semi-supervised learning of lexicons


• Advantages:
  – Can be domain-specific
  – Can be more robust (more words)

• Intuition
  – Start with a seed set of words (‘good’, ‘poor’)
  – Find other words that have similar polarity:
    ● Using “and” and “but”
    ● Using words that occur nearby in the same document
    ● Using WordNet synonyms and antonyms

* Algoritmhs: Hatzivassiloglou & McKeown, Turney, WordNet.


#-------------------------------------------------------------------------------
# 5.5) Other Sentiment Tasks (11:01)
#-------------------------------------------------------------------------------

• Finding aspect/attribute/target of sentiment

• Unbalanced classes

• Non-binary classes
   

  
SUMMARY ON SENTIMENT

• Generally modeled as classification or regression task
  – predict a binary or ordinal label (rating 1-5)

• Features:
  – Negation is important
  – Using all words (in Naive bayes) works well for some tasks
  – Finding subsets of words may help in other tasks
    ● Hand-built polarity lexicons
    ● Use seeds and semi-supervised learning to induce lexicons


############################################################################
#
# References
#
############################################################################


[2.2] Estimating N-gram Probabilities (9:38)

[2.2.1] Language Modeling Toolkits
	SRILM – http://www.speech.sri.com/projects/srilm/

[2.2.2] Google Book N-grams - http://ngrams.googlelabs.com/

[3.1] Some spelling error test sets
[3.1.1] Wikipedia’s list of common English misspelling - http://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines
[3.1.2] Aspell filtered version of that list - http://aspell.net/test/
[3.1.3] Birkbeck spelling err corpus - http://www.ota.ox.ac.uk/headers/0643.xml
[3.1.4] Peter Norvig list of errors - http://norvig.com/ngrams/spell-errors.txt
[3.1.5] Peter Norvig’s list of counts of single-edit errors - http://norvig.com/ngrams/spell-errors.txt


[5.3.1] The General Inquirer, List of Categories - http://www.wjh.harvard.edu/~inquirer/homecat.htm

[5.3.2] LIWC (Linguistic Inquiry and Word Count) - http://www.liwc.net/
